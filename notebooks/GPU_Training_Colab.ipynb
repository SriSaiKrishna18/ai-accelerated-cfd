{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Navier-Stokes AI Model Training (GPU)\n",
                "\n",
                "**AI-HPC Hybrid Project**\n",
                "\n",
                "## For Kaggle:\n",
                "1. Add your dataset: `/kaggle/input/hybrid-ai-hpc/`\n",
                "2. Enable GPU: Settings → Accelerator → GPU\n",
                "3. Run all cells\n",
                "\n",
                "**Estimated time: 30-45 minutes for 50 epochs**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CONFIGURATION - CHANGE PATHS HERE FOR KAGGLE\n",
                "# ============================================\n",
                "\n",
                "# For Kaggle: Use your dataset path\n",
                "TRAIN_DATA_PATH = '/kaggle/input/hybrid-ai-hpc/train_data.npz'\n",
                "VAL_DATA_PATH = '/kaggle/input/hybrid-ai-hpc/val_data.npz'\n",
                "\n",
                "# For Colab: Upload files and use local paths\n",
                "# TRAIN_DATA_PATH = 'train_data.npz'\n",
                "# VAL_DATA_PATH = 'val_data.npz'\n",
                "\n",
                "# Training configuration\n",
                "CONFIG = {\n",
                "    'epochs': 50,\n",
                "    'batch_size': 8,\n",
                "    'learning_rate': 1e-3,\n",
                "    'seq_len': 5,\n",
                "    'pred_len': 10,\n",
                "    'hidden_dims': [64, 64, 64],\n",
                "}\n",
                "\n",
                "print(\"Data paths:\")\n",
                "print(f\"  Train: {TRAIN_DATA_PATH}\")\n",
                "print(f\"  Val: {VAL_DATA_PATH}\")\n",
                "print(\"\\nConfiguration:\")\n",
                "for k, v in CONFIG.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Define Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "\n",
                "class ConvLSTMCell(nn.Module):\n",
                "    \"\"\"Convolutional LSTM Cell.\"\"\"\n",
                "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
                "        super().__init__()\n",
                "        self.hidden_dim = hidden_dim\n",
                "        padding = kernel_size // 2\n",
                "        \n",
                "        self.conv = nn.Conv2d(\n",
                "            input_dim + hidden_dim, 4 * hidden_dim,\n",
                "            kernel_size=kernel_size, padding=padding, bias=True\n",
                "        )\n",
                "    \n",
                "    def forward(self, x, hidden_state):\n",
                "        h_cur, c_cur = hidden_state\n",
                "        combined = torch.cat([x, h_cur], dim=1)\n",
                "        gates = self.conv(combined)\n",
                "        \n",
                "        i, f, o, g = torch.chunk(gates, 4, dim=1)\n",
                "        i = torch.sigmoid(i)\n",
                "        f = torch.sigmoid(f)\n",
                "        o = torch.sigmoid(o)\n",
                "        g = torch.tanh(g)\n",
                "        \n",
                "        c_next = f * c_cur + i * g\n",
                "        h_next = o * torch.tanh(c_next)\n",
                "        \n",
                "        return h_next, (h_next, c_next)\n",
                "    \n",
                "    def init_hidden(self, batch_size, height, width, device):\n",
                "        h = torch.zeros(batch_size, self.hidden_dim, height, width, device=device)\n",
                "        c = torch.zeros(batch_size, self.hidden_dim, height, width, device=device)\n",
                "        return (h, c)\n",
                "\n",
                "\n",
                "class ConvLSTM(nn.Module):\n",
                "    \"\"\"Multi-layer ConvLSTM for fluid flow prediction.\"\"\"\n",
                "    def __init__(self, input_dim=3, hidden_dims=[64, 64, 64], kernel_size=3):\n",
                "        super().__init__()\n",
                "        self.input_dim = input_dim\n",
                "        self.hidden_dims = hidden_dims\n",
                "        self.num_layers = len(hidden_dims)\n",
                "        \n",
                "        self.input_conv = nn.Conv2d(input_dim, hidden_dims[0], 3, padding=1)\n",
                "        \n",
                "        self.cells = nn.ModuleList()\n",
                "        for i in range(self.num_layers):\n",
                "            in_dim = hidden_dims[i]\n",
                "            self.cells.append(ConvLSTMCell(in_dim, hidden_dims[i], kernel_size))\n",
                "        \n",
                "        self.output_conv = nn.Conv2d(hidden_dims[-1], input_dim, 1)\n",
                "    \n",
                "    def forward(self, x, future_steps=1, hidden_state=None):\n",
                "        batch_size, channels, height, width = x.shape\n",
                "        device = x.device\n",
                "        \n",
                "        if hidden_state is None:\n",
                "            hidden_state = [cell.init_hidden(batch_size, height, width, device) \n",
                "                          for cell in self.cells]\n",
                "        \n",
                "        outputs = []\n",
                "        current_input = x\n",
                "        \n",
                "        for step in range(future_steps):\n",
                "            h = self.input_conv(current_input)\n",
                "            \n",
                "            for i, cell in enumerate(self.cells):\n",
                "                h, hidden_state[i] = cell(h, hidden_state[i])\n",
                "            \n",
                "            output = self.output_conv(h)\n",
                "            outputs.append(output)\n",
                "            current_input = output\n",
                "        \n",
                "        return torch.stack(outputs, dim=1)\n",
                "\n",
                "\n",
                "print(\"Models defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NumpyDataset(Dataset):\n",
                "    \"\"\"Dataset loader for numpy data files.\"\"\"\n",
                "    def __init__(self, data_path, seq_len=5, pred_len=10):\n",
                "        self.seq_len = seq_len\n",
                "        self.pred_len = pred_len\n",
                "        \n",
                "        data = np.load(data_path)\n",
                "        u = data['u']\n",
                "        v = data['v']\n",
                "        p = data['p']\n",
                "        \n",
                "        self.data = np.stack([u, v, p], axis=1).astype(np.float32)\n",
                "        self.num_sequences = max(1, len(self.data) - seq_len - pred_len + 1)\n",
                "        \n",
                "        print(f\"Loaded {data_path}: {self.data.shape}\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return self.num_sequences\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        start = idx\n",
                "        mid = start + self.seq_len\n",
                "        end = mid + self.pred_len\n",
                "        \n",
                "        input_seq = self.data[start:mid]\n",
                "        target_seq = self.data[mid:end]\n",
                "        \n",
                "        return torch.from_numpy(input_seq), torch.from_numpy(target_seq)\n",
                "\n",
                "\n",
                "print(\"Dataset class defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if files exist\n",
                "import os\n",
                "\n",
                "print(\"Checking data files...\")\n",
                "if os.path.exists(TRAIN_DATA_PATH):\n",
                "    print(f\"  Found: {TRAIN_DATA_PATH}\")\n",
                "else:\n",
                "    print(f\"  NOT FOUND: {TRAIN_DATA_PATH}\")\n",
                "    print(\"  Please check your dataset path!\")\n",
                "\n",
                "if os.path.exists(VAL_DATA_PATH):\n",
                "    print(f\"  Found: {VAL_DATA_PATH}\")\n",
                "else:\n",
                "    print(f\"  NOT FOUND: {VAL_DATA_PATH}\")\n",
                "    print(\"  Please check your dataset path!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Load datasets using the paths from CONFIG\n",
                "train_dataset = NumpyDataset(TRAIN_DATA_PATH, CONFIG['seq_len'], CONFIG['pred_len'])\n",
                "val_dataset = NumpyDataset(VAL_DATA_PATH, CONFIG['seq_len'], CONFIG['pred_len'])\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
                "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
                "\n",
                "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
                "print(f\"Val batches: {len(val_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create model\n",
                "model = ConvLSTM(input_dim=3, hidden_dims=CONFIG['hidden_dims']).to(device)\n",
                "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "\n",
                "# Setup training\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
                "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
                "criterion = nn.MSELoss()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, criterion, device):\n",
                "    model.train()\n",
                "    total_loss = 0.0\n",
                "    \n",
                "    for inputs, targets in loader:\n",
                "        inputs = inputs.to(device)\n",
                "        targets = targets.to(device)\n",
                "        \n",
                "        initial_state = inputs[:, -1]\n",
                "        pred_len = targets.shape[1]\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        predictions = model(initial_state, future_steps=pred_len)\n",
                "        loss = criterion(predictions, targets)\n",
                "        loss.backward()\n",
                "        \n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "\n",
                "def validate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    total_loss = 0.0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for inputs, targets in loader:\n",
                "            inputs = inputs.to(device)\n",
                "            targets = targets.to(device)\n",
                "            \n",
                "            initial_state = inputs[:, -1]\n",
                "            pred_len = targets.shape[1]\n",
                "            \n",
                "            predictions = model(initial_state, future_steps=pred_len)\n",
                "            loss = criterion(predictions, targets)\n",
                "            total_loss += loss.item()\n",
                "    \n",
                "    return total_loss / len(loader)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "import time\n",
                "\n",
                "train_losses = []\n",
                "val_losses = []\n",
                "best_val_loss = float('inf')\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Starting Training\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "start_time = time.time()\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    epoch_start = time.time()\n",
                "    \n",
                "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
                "    val_loss = validate(model, val_loader, criterion, device)\n",
                "    \n",
                "    train_losses.append(train_loss)\n",
                "    val_losses.append(val_loss)\n",
                "    \n",
                "    scheduler.step(val_loss)\n",
                "    current_lr = optimizer.param_groups[0]['lr']\n",
                "    \n",
                "    epoch_time = time.time() - epoch_start\n",
                "    \n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'train_loss': train_loss,\n",
                "            'val_loss': val_loss,\n",
                "        }, 'best_model.pth')\n",
                "        print(f\"Epoch {epoch:3d}/{CONFIG['epochs']} | Train: {train_loss:.6f} | Val: {val_loss:.6f} | LR: {current_lr:.2e} | Time: {epoch_time:.1f}s | * BEST *\")\n",
                "    else:\n",
                "        print(f\"Epoch {epoch:3d}/{CONFIG['epochs']} | Train: {train_loss:.6f} | Val: {val_loss:.6f} | LR: {current_lr:.2e} | Time: {epoch_time:.1f}s\")\n",
                "\n",
                "total_time = time.time() - start_time\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"Training Complete!\")\n",
                "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
                "print(f\"Best val loss: {best_val_loss:.6f}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save training history\n",
                "np.savez('history.npz', train_loss=train_losses, val_loss=val_losses)\n",
                "print(\"Training history saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "epochs = range(1, len(train_losses) + 1)\n",
                "\n",
                "ax1.plot(epochs, train_losses, 'b-', label='Training', linewidth=2)\n",
                "ax1.plot(epochs, val_losses, 'r-', label='Validation', linewidth=2)\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('MSE Loss')\n",
                "ax1.set_title('Training History')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "ax2.plot(epochs, train_losses, 'b-', label='Training', linewidth=2)\n",
                "ax2.plot(epochs, val_losses, 'r-', label='Validation', linewidth=2)\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('MSE Loss (log)')\n",
                "ax2.set_title('Training History (Log Scale)')\n",
                "ax2.set_yscale('log')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_history.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prediction visualization\n",
                "model.eval()\n",
                "\n",
                "inputs, targets = val_dataset[0]\n",
                "inputs = inputs.unsqueeze(0).to(device)\n",
                "targets = targets.numpy()\n",
                "\n",
                "with torch.no_grad():\n",
                "    initial_state = inputs[:, -1]\n",
                "    predictions = model(initial_state, future_steps=10)\n",
                "    predictions = predictions[0].cpu().numpy()\n",
                "\n",
                "step = 4\n",
                "fig, axes = plt.subplots(3, 3, figsize=(14, 12))\n",
                "\n",
                "fields = ['u-velocity', 'v-velocity', 'pressure']\n",
                "for i, name in enumerate(fields):\n",
                "    pred = predictions[step, i]\n",
                "    true = targets[step, i]\n",
                "    diff = pred - true\n",
                "    \n",
                "    vmin, vmax = true.min(), true.max()\n",
                "    \n",
                "    im1 = axes[i, 0].imshow(true, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
                "    axes[i, 0].set_title(f'{name} (Ground Truth)')\n",
                "    plt.colorbar(im1, ax=axes[i, 0])\n",
                "    \n",
                "    im2 = axes[i, 1].imshow(pred, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
                "    axes[i, 1].set_title(f'{name} (AI Prediction)')\n",
                "    plt.colorbar(im2, ax=axes[i, 1])\n",
                "    \n",
                "    diff_max = max(abs(diff.min()), abs(diff.max()))\n",
                "    im3 = axes[i, 2].imshow(diff, cmap='RdBu_r', vmin=-diff_max, vmax=diff_max)\n",
                "    axes[i, 2].set_title(f'{name} (Error)')\n",
                "    plt.colorbar(im3, ax=axes[i, 2])\n",
                "\n",
                "plt.suptitle(f'AI vs Ground Truth (Step {step+1})', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('field_comparison.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute final metrics\n",
                "rmse_u = np.sqrt(np.mean((predictions[:, 0] - targets[:, 0])**2))\n",
                "rmse_v = np.sqrt(np.mean((predictions[:, 1] - targets[:, 1])**2))\n",
                "rmse_p = np.sqrt(np.mean((predictions[:, 2] - targets[:, 2])**2))\n",
                "combined_rmse = np.sqrt(rmse_u**2 + rmse_v**2 + rmse_p**2)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"FINAL METRICS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"u-velocity RMSE: {rmse_u:.6f}\")\n",
                "print(f\"v-velocity RMSE: {rmse_v:.6f}\")\n",
                "print(f\"pressure RMSE:   {rmse_p:.6f}\")\n",
                "print(f\"\\nCombined RMSE:   {combined_rmse:.6f}\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "if combined_rmse < 0.05:\n",
                "    print(\"Rating: EXCELLENT\")\n",
                "elif combined_rmse < 0.1:\n",
                "    print(\"Rating: GOOD\")\n",
                "else:\n",
                "    print(\"Rating: NEEDS IMPROVEMENT\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Outputs\n",
                "\n",
                "On Kaggle, outputs are saved to `/kaggle/working/`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List output files\n",
                "import os\n",
                "\n",
                "print(\"Output files saved:\")\n",
                "for f in ['best_model.pth', 'history.npz', 'training_history.png', 'field_comparison.png']:\n",
                "    if os.path.exists(f):\n",
                "        size = os.path.getsize(f) / 1024\n",
                "        print(f\"  {f}: {size:.1f} KB\")\n",
                "\n",
                "print(\"\\nDownload from Kaggle: Output tab → Download All\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kaggle": {
            "accelerator": "gpu",
            "dataSources": [
                {
                    "datasetId": "hybrid-ai-hpc"
                }
            ]
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}